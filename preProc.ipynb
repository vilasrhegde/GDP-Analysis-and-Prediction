{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the DataSet 🚨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('all_countries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We came to know that this dataset has 227 rows and 20 columns.\n",
    "> Now we need more information about in which datatype they are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. Country and Region are the only 2 columns which are non-numerical\n",
    "2. In those numerical columns, GDP ($ per capita) is the only one which has float values\n",
    "3. Population and Area are integers\n",
    "4. Remaining all are in object type, which doesn't help us in this format\n",
    "5. We will convert all object type columns into float\n",
    "6. We need to shorten the column names also, which is seems longgggg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Changing the names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=([\"country\",\"region\",\"population\",\"area\",\"density\",\"coastline_area_ratio\",\"net_migration\",\"infant_mortality\",\"gdp_per_capita\",\n",
    "                  \"literacy\",\"phones\",\"arable\",\"crops\",\"other\",\"climate\",\"birthrate\",\"deathrate\",\"agriculture\",\"industry\",\n",
    "                  \"service\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing datatypes\n",
    "- converting into float/string types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country = data.country.astype('category')\n",
    "data.region = data.region.astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will first convert obj -> str then str -> float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.density = data.density.astype(str)\n",
    "data.density = data.density.str.replace(',','.').astype(float)\n",
    "\n",
    "data.coastline_area_ratio = data.coastline_area_ratio.astype(str)\n",
    "data.coastline_area_ratio = data.coastline_area_ratio.str.replace(',','.').astype(float)\n",
    "\n",
    "data.net_migration = data.net_migration.astype(str)\n",
    "data.net_migration = data.net_migration.str.replace(',','.').astype(float)\n",
    "\n",
    "data.infant_mortality = data.infant_mortality.astype(str)\n",
    "data.infant_mortality = data.infant_mortality.str.replace(',','.').astype(float)\n",
    "\n",
    "data.literacy = data.literacy.astype(str)\n",
    "data.literacy = data.literacy.str.replace(',','.').astype(float)\n",
    "\n",
    "data.phones = data.phones.astype(str)\n",
    "data.phones = data.phones.str.replace(',','.').astype(float)\n",
    "\n",
    "data.service = data.service.astype(str)\n",
    "data.service = data.service.str.replace(',','.').astype(float)\n",
    "\n",
    "data.industry = data.industry.astype(str)\n",
    "data.industry = data.industry.str.replace(',','.').astype(float)\n",
    "\n",
    "data.agriculture = data.agriculture.astype(str)\n",
    "data.agriculture = data.agriculture.str.replace(',','.').astype(float)\n",
    "\n",
    "data.deathrate = data.deathrate.astype(str)\n",
    "data.deathrate = data.deathrate.str.replace(',','.').astype(float)\n",
    "\n",
    "data.birthrate = data.birthrate.astype(str)\n",
    "data.birthrate = data.birthrate.str.replace(',','.').astype(float)\n",
    "\n",
    "data.climate = data.climate.astype(str)\n",
    "data.climate = data.climate.str.replace(',','.').astype(float)\n",
    "\n",
    "data.arable = data.arable.astype(str)\n",
    "data.arable = data.arable.str.replace(',','.').astype(float)\n",
    "\n",
    "data.crops = data.crops.astype(str)\n",
    "data.crops = data.crops.str.replace(',','.').astype(float)\n",
    "\n",
    "data.other = data.other.astype(str)\n",
    "data.other = data.other.str.replace(',','.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now all seems okay to proceed for statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data.columns,x=data.columns,y=data.isnull().sum(),title=\"Number of missing values\",text_auto='.2s')\n",
    "fig.update_traces(textfont_size=12,textangle=0,textposition=\"outside\",cliponaxis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.imshow(data.isnull())\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Climate column has maximum missing values\n",
    "- Initial 6 columns have no missing values!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Data Validity ✅\n",
    "We will randomly select some attributes to compare on internet to check whether are they believable or not"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (p) => population\n",
    "- (a) => area\n",
    "- (c) => coastline/Area\n",
    "- (g) => GDP Source information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > https://www.jetpunk.com/info/countries-by-coastline\n",
    "- > https://en.wikipedia.org/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internet data:\n",
    "1. Afghanistan\n",
    "- (p) = ~ 40M\n",
    "- (a) = ~ 6L /km2\n",
    "- (c) => 0\n",
    "- (g) => ~ 1400 crores USD\n",
    "\n",
    "2. France\n",
    "- (p) = ~ 66M\n",
    "- (a) = ~ 5L /km2\n",
    "- (c) => 7.58m/km2\n",
    "- (g) => ~ 2.96 lakh crore USD\n",
    "\n",
    "3. Spain\n",
    "- (p) = ~ 4.7 crores\n",
    "- (a) = ~ 5L /km2   \n",
    "- (c) => 7.03m/km2\n",
    "- (g) => ~ 1.43 lakh crore USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[[0,69,190],['country','population','area','coastline_area_ratio','gdp_per_capita']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "- Almost looks near to actual values with the data we have\n",
    "- altough there are slight dissimilarities quite a few, but not much to ruin our analysis\n",
    "-  data points are seems OLD, hence not updated to this date such as population,areas etc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying undefined features 🔍\n",
    " \n",
    "### what does the attributes such as climate,aggriculure, industry and service refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,['country','region','climate','agriculture','industry','service']].tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- If we add agriculture,industry and service, the sum becomes 1\n",
    "- Which means they are percentage(%) values (9+28+63 = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> understaning climate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(data.climate.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one=data.loc[:, ['country', 'region', 'climate']][data.climate == 1].head()\n",
    "two=data.loc[:, ['country', 'region', 'climate']][data.climate == 1.5].head()\n",
    "three=data.loc[:, ['country', 'region', 'climate']][data.climate == 2].head()\n",
    "four=data.loc[:, ['country', 'region', 'climate']][data.climate == 2.5].head()\n",
    "five=data.loc[:, ['country', 'region', 'climate']][data.climate == 3].head()\n",
    "six=data.loc[:, ['country', 'region', 'climate']][data.climate == 4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([one,two,three,four,five,six])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. Climate has 6 unique values along with  missing ones too\n",
    "2. we can assume countries with climate value as 1 are having more desert in them (Afghanistan,Australia...)\n",
    "3. Tropical => climate 2\n",
    "4. cold/cool => cilmate 3\n",
    "5. hot + tropical => climate 1.5\n",
    "6. tropical + cold => climate 2.5\n",
    "7. Countries having climate 4 are also can be added to cold/cool, but it is nowhere mentioned in the dataset, we will deal with this later\n",
    "8. 226 - 194 = 22\n",
    "means there are 22 null values present, so these must be replaced by 0 or something new value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning 🧹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. net_migration, infant_mortality only 3 values missing, which  belong to small nations so we can  fill as 0\n",
    "2. West Sahara country's gdp_per_capita is missing, as per the internet it is showingn $2500, we will replace by this value\n",
    "3. literacy has 18, we fill them by its mean value\n",
    "4. phone will be replaced by mean of it\n",
    "5. arable,crops and other have 4,2,2 missing respectively, each will be replaced by 0\n",
    "6. Climate has maximum number of missing  values (22), will be replaced by 0 (unknown climate)\n",
    "7. birthrate and deathrate are calucated per 1000, not population based. so these can be updated by mean value only\n",
    "8. Agriculture, service and industry have 15-16 missing values.\n",
    "All belong to smaller nations which are heavily dependant upon service and less on agriculture and industry,So\n",
    "- agriculture => 0.15\n",
    "- industry => 0.05\n",
    "- service => 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['net_migration'].fillna(0,inplace=True)\n",
    "data['infant_mortality'].fillna(0,inplace=True)\n",
    "data['arable'].fillna(0,inplace=True)\n",
    "data['crops'].fillna(0,inplace=True)\n",
    "data['other'].fillna(0,inplace=True)\n",
    "data['climate'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gdp_per_capita'].fillna(2500,inplace=True)\n",
    "data['literacy'].fillna(data.groupby('region')['literacy'].transform('mean'),inplace=True)\n",
    "data['phones'].fillna(data.groupby('region')['phones'].transform('mean'),inplace=True)\n",
    "data['birthrate'].fillna(data.groupby('region')['birthrate'].transform('mean'),inplace=True)\n",
    "data['deathrate'].fillna(data.groupby('region')['deathrate'].transform('mean'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['agriculture'].fillna(0.17,inplace=True)\n",
    "data['service'].fillna(0.8,inplace=True)\n",
    "data['industry'].fillna((1-data['agriculture'] - data['service']),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) 🔭"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Understanding the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.imshow(data.corr(),text_auto=True, aspect=\"auto\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Strong correlations are,\n",
    "    1. infant_mortality & birthrate\n",
    "    2. infant_mortality & literacy\n",
    "    3. gdp_per_capita & phones\n",
    "    4. arable & other than crops\n",
    "    5. birthrate & literacy (less literacy = higher the birthrate)\n",
    "- Weak correlations are,\n",
    "    1. infant_mortality & agriculture\n",
    "    2. birthrate & phones\n",
    "    3. gdp_per_capita & birthrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going in depth with more features simultaneously side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter_matrix(data, dimensions=data[['population', 'area', 'net_migration', 'gdp_per_capita', 'climate']],width=700, height=720,title=\"Features relationships\",color=\"gdp_per_capita\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- net_migration & gdp_per_capita has good correlations, which means migrants always prefers to move to the countries having better economy and growth which is gdp in our case.\n",
    "- climate and populations are less correlated, means people avoid extreme weather and climate places\n",
    "- as area increased the amount of migratants also increased, obvious."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Analysis on multiple features 🔬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization\n",
    "data.to_csv('filtered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(data,x=\"area\",y=\"gdp_per_capita\",points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the area increased, gdp did not kept up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.bar(data, x='region', y='country')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-Saharan Africa region has the most countries in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data, x='region', y='gdp_per_capita',color='country',title=\"GDP of multiple Regions\",width=700,height=500)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Western Europe and Latin Amer. & Carib has highest GDP respectively, where as North Africa nad Balitics have least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter_3d(data, x='region', y='gdp_per_capita', z='net_migration',color='region',size=\"gdp_per_capita\",height=700)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migrants are dense towards Asia,North America and North Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(data, x='region', y='phones', z='literacy',\n",
    "              color='region', size='population', size_max=18,\n",
    "              symbol='region', opacity=0.7)\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As literacy increased there is many points of phones, and asia has highest population also which makes belive us that chance of having phones are high here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data, x=\"area\", y=\"region\", color=\"region\", title=\"Area of each region\",width=1000,height=500,orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected Asia and Sub-saharan africa have highest area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensive GDP Analysis ⚒️"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the regional ranking according to the average gdp_per_capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data.groupby('region')['gdp_per_capita'].mean().sort_values()\n",
    "\n",
    "fig = px.bar(d, x='gdp_per_capita',orientation='h',width=700,height=500)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- North America and Western Europe have highest average of gdp_per_capita\n",
    "- Sub-Saharan Africa and C.W of Ind States have least; Which means large migration happened in the last decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(data, x=\"literacy\", y=\"gdp_per_capita\",title='GDP Analysis v/s Literacy', size='literacy', color=\"region\",\n",
    "           hover_name=\"country\", log_x=True, size_max=60)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this observation it became crystal clear that GDP of a country is highly dependant upon literacy and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data, x=\"arable\", y=\"gdp_per_capita\",color='area', title='GDP v/s Arable land analysis',marginal_x=\"histogram\", marginal_y=\"rug\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see strong relation between GDP and Arable land. So agriculture is not a strong factor anymore for economy of a country according to this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.line(data, x='infant_mortality', y='gdp_per_capita',color='region',width=700,height=700,title='GDP v/s Infant Mortality Rate',symbol='region')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can observe that poor countries are suffering heavy loss of infants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data, x=\"agriculture\", y=\"gdp_per_capita\", color=\"region\",\n",
    "                 title='GDP v/s Agriculture (Crops)')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it is shown as poor countries are more dependant upon harvesting crops than developed countris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data, x=\"industry\", y=\"gdp_per_capita\",title='GDP Analysis v/s Industry', size='literacy', color=\"region\",\n",
    "           hover_name=\"country\", log_x=True, size_max=60)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not industry as our parameter because it is evenly distributed across all the countries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Conditioning ⚖️"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training data to feed the machine learning model.\n",
    "\n",
    "We are going to perform the following:\n",
    "1. Convert `region` column into numerical values.\n",
    "2. Splitting the dataset into `train` and `test` in the ratio of 80 : 20\n",
    "3. We will drop the column `countries` because it is in string.\n",
    "4. We are using `gdp_per_capita` as labels.\n",
    "5. We repeat the train test split with different ratios for better splitting with/without feature selection/scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming region column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.concat([data,pd.get_dummies(data['region'], prefix='region')],axis=1).drop(['region'],axis=1)\n",
    "print(data_new.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split 1: full data without Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data_new['gdp_per_capita']\n",
    "X=data_new.drop(['gdp_per_capita','country'],axis=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split 2: full data with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "\n",
    "X2_train = sc.fit_transform(X_train)\n",
    "X2_test = sc.fit_transform(X_test)\n",
    "y2_train = y_train\n",
    "y2_test = y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split 3: selected features data without Scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We select only some proportion of our features which have `correlation` score near to `+/- 0.3` with `gdp_per_capita`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y\n",
    "X3 = data_new.drop(['gdp_per_capita','country','population', 'area', 'coastline_area_ratio', 'arable',\n",
    "                      'crops', 'other', 'climate', 'deathrate', 'industry'], axis=1)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3.tail()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split 4: selected features data with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train = sc.fit_transform(X3_train)\n",
    "X4_test = sc.fit_transform(X3_test)\n",
    "y4_train = y3_train\n",
    "y4_test = y3_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression 📈"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first trial towards achieving better results with the supervised machine learning algorthms. As we observed before, some of the features in our dataset were not correlated at each other. Although we are going to test it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model_1 = LinearRegression()\n",
    "linear_model_1.fit(X_train,y_train)\n",
    "\n",
    "linear_model_2 = LinearRegression()\n",
    "linear_model_2.fit(X2_train,y2_train)\n",
    "\n",
    "linear_model_3 = LinearRegression()\n",
    "linear_model_3.fit(X3_train,y3_train)\n",
    "\n",
    "linear_model_4 = LinearRegression()\n",
    "linear_model_4.fit(X4_train,y4_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Predictions of all the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1_pred = linear_model_1.predict(X_test)\n",
    "lm2_pred = linear_model_2.predict(X2_test)\n",
    "lm3_pred = linear_model_3.predict(X3_test)\n",
    "lm4_pred = linear_model_4.predict(X4_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('The Performance of the Linear Regression:')\n",
    "\n",
    "print('\\nAll features without Scaling:')\n",
    "print('MeanAbsoluteError:', metrics.mean_absolute_error(y_test, lm1_pred))\n",
    "print('MeanSquaredError:', np.sqrt(metrics.mean_squared_error(y_test, lm1_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y_test, lm1_pred))\n",
    "\n",
    "print('\\nAll features with Scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y2_test, lm2_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y2_test, lm2_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y2_test, lm2_pred))\n",
    "\n",
    "print('\\nOnly Selected features without Scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y3_test, lm3_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y3_test, lm3_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y3_test, lm3_pred))\n",
    "\n",
    "print('\\nOnly Selected features  with Scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y4_test, lm4_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y4_test, lm4_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y4_test, lm4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter( x=y4_test, y=lm4_pred,title=\"Test vs LR's Predictions\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a decent result from the Linear Regression with feature selection and scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine) 🩼"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm1 = SVR(kernel='rbf')\n",
    "svm1.fit(X_train,y_train)\n",
    "\n",
    "svm2 = SVR(kernel='rbf')\n",
    "svm2.fit(X2_train,y2_train)\n",
    "\n",
    "svm3 = SVR(kernel='rbf')\n",
    "svm3.fit(X3_train,y3_train)\n",
    "\n",
    "svm4 = SVR(kernel='rbf')\n",
    "svm4.fit(X4_train,y4_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1_pred = svm1.predict(X_test)\n",
    "svm2_pred = svm2.predict(X2_test)\n",
    "svm3_pred = svm3.predict(X3_test)\n",
    "svm4_pred = svm4.predict(X4_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Performance:')\n",
    "\n",
    "print('\\nall features, No scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, svm1_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm1_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y_test, svm1_pred))\n",
    "\n",
    "print('\\nall features, with scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y2_test, svm2_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y2_test, svm2_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y2_test, svm2_pred))\n",
    "\n",
    "print('\\nselected features, No scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y3_test, svm3_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y3_test, svm3_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y3_test, svm3_pred))\n",
    "\n",
    "print('\\nselected features, with scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y4_test, svm4_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y4_test, svm4_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y4_test, svm4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter( x=y3_test, y=svm3_pred,title='Test vs SVM Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling and selection did not help much for the prediction in our case, hence results of SVM is worse than LR. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 🌲"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will feed our standard data which has been splitted, and will not going to perform any scaling or selection of features because these are not going to improve prediction by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf1 = RandomForestRegressor(random_state=11, n_estimators=200)\n",
    "rf3 = RandomForestRegressor(random_state=11, n_estimators=200)\n",
    "\n",
    "rf1.fit(X_train, y_train)\n",
    "rf3.fit(X3_train, y3_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1_pred = rf1.predict(X_test)\n",
    "rf3_pred = rf3.predict(X3_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Performance:')\n",
    "\n",
    "print('\\nall features, No scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, rf1_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rf1_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y_test, rf1_pred))\n",
    "\n",
    "print('\\nselected features, No scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y3_test, rf3_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y3_test, rf3_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y3_test, rf3_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter( x=y_test, y=rf1_pred,title='Test vs RF Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is good, but we will try to improve its performance by following grid search method to get understanding of good parameter values that can improvise the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Parameters we are going to test are,\n",
    "- `n-estimators`: The number of trees in the forest. Default 100 or 10 in new versions. \n",
    "- `min_sample_leaf`: The minimum number of samples required to be at leaf node.\n",
    "- `max_features`: The number of features that we are looking for the Best split\n",
    "- `bootstrap`: While building a tree we give bootstrap samples or whole dataset for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {'max_features': ['sqrt', 'log2','float','auto'],\n",
    "              'min_samples_leaf': [1, 3, 5],\n",
    "              'n_estimators': [100, 500, 1000],\n",
    "             'bootstrap': [False, True]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rf_grid = GridSearchCV(estimator= RandomForestRegressor(), param_grid = rf_param_grid,  n_jobs=-1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_predictions = rf_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, rf_grid_predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rf_grid_predictions)))\n",
    "print('R2_Score: ', metrics.r2_score(y_test, rf_grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter( x=y_test, y=rf_grid_predictions,title='Test vs RF GridSearched Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y3_test, rf3_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see a significant improvement over initial parameter. Probably initial values are only optimum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pickle\n",
    "pickle.dump(rf3,open('RFmodel.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting 🌈"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm1 = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_depth=3,\n",
    "                                 subsample=1.0, max_features= None, random_state=101)\n",
    "gbm3 = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_depth=3,\n",
    "                                 subsample=1.0, max_features= None, random_state=101)\n",
    "\n",
    "gbm1.fit(X_train, y_train)\n",
    "gbm3.fit(X3_train, y3_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm1_pred = gbm1.predict(X_test)\n",
    "gbm3_pred = gbm3.predict(X3_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradiant Boosting Performance:')\n",
    "\n",
    "print('\\nall features, No scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, gbm1_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, gbm1_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y_test, gbm1_pred))\n",
    "\n",
    "print('\\nselected features, No scaling:')\n",
    "print('MAE:', metrics.mean_absolute_error(y3_test, gbm3_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y3_test, gbm3_pred)))\n",
    "print('R2_Score: ', metrics.r2_score(y3_test, gbm3_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter( x=y_test, y=gbm1_pred,title='Test vs GBR Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting gave us pretty good performance overall, that too without the need of optimisation!\n",
    "> Although RandomForest and GradientBoosting are comparable with respect to our same dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the importance across all the Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = pd.Series(gbm1.feature_importances_, list(X_train)).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(imp_features, title=\"Importance of Features\",color='value',height=700,width=700,orientation='v')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:👀\n",
    "- Number of `phones` seems to having more power for prediction\n",
    "- Rest all the features are none the less similar contribution for the performance of this model.\n",
    "- We achieved `R2_Score` of 0.82 from `GBR` and 0.73 from `RandomForest` which means GBR can be better over RF\n",
    "- We will give a shot to even optimise this model (GBR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation of GBM 🚀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are once again going to use GridSearch method to pick better parameters for our regression model. GB is very robust towards handling over-fitting, so large data results in better output accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters we are using for optimisation are,\n",
    "1. `n-estimators`: The number of boosting stages to perform.\n",
    "2. `learning_rate`: It is the shrinking of contribution of each tree\n",
    "3. `max_depth`: Maximum depth of individual regression estimators (nodes)\n",
    "4. `subsample`: The fraction of samples to be used for fitting the individual base learners. A subsample = 0.5 means that 50% of training data is used prior to growing a tree.\n",
    "5. `min_sample_leaf`: The minimum number of samples required to consider a leaf node\n",
    "6. `min_sample_split`: The minimum number required to split an internal node\n",
    "7. `max_features`: Maximum number of features to consider while looking for the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {'learning_rate':[1,0.1, 0.01, 0.001], \n",
    "           'n_estimators':[100, 500, 1000],\n",
    "          'max_depth':[3, 5, 8],\n",
    "          'subsample':[0.7, 1], \n",
    "          'min_samples_leaf':[1, 20],\n",
    "          'min_samples_split':[10, 20],\n",
    "          'max_features':[4, 7]}\n",
    "\n",
    "gbm_tuning = GridSearchCV(estimator =GradientBoostingRegressor(random_state=11),\n",
    "                          param_grid = gbm_param_grid,\n",
    "                          n_jobs=-1,\n",
    "                          cv=5)\n",
    "\n",
    "gbm_tuning.fit(X_train,y_train)\n",
    "print(gbm_tuning.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid_predictions = gbm_tuning.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, gbm_grid_predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, gbm_grid_predictions)))\n",
    "print('R2_Score: ', metrics.r2_score(y_test, gbm_grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter( x=y_test, y=gbm_grid_predictions,title='Test vs Optimised GBR Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not up to the mark interestingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_opt = GradientBoostingRegressor(learning_rate=0.01, n_estimators=500,max_depth=5, min_samples_split=10, min_samples_leaf=1, \n",
    "                                    subsample=0.7,max_features=7, random_state=101)\n",
    "gbm_opt.fit(X_train,y_train)\n",
    "feat_imp2 = pd.Series(gbm_opt.feature_importances_, list(X_train)).sort_values(ascending=False)\n",
    "\n",
    "fig = px.bar(feat_imp2, title=\"Importance of Features\",color='value',height=700,width=700,orientation='v')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations: 👀\n",
    "- Optimisation caused decrease in performance of our model surprisingly\n",
    "- This happened because the limitaions of processing of GridSearch\n",
    "- But this resulted in a difference in importance of features\n",
    "- We can consider the performance of both RandomForest and GeadientBoosting are quite same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Awards! 🏆"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. _`Random Forest` with Feature selection and NO scaling_\n",
    "- Mean Absolute Error __(MAE)__: 2451.88\n",
    "- Root Mean Squared Error __(RMSE)__: 3580.53\n",
    "- R-Squared Score __(R2_Score)__: 0.84\n",
    "\n",
    "2. _`Gradient Boosting` with selected features and scaling_\n",
    "- Mean Absolute Error __(MAE)__: 2467.21\n",
    "- Root Mean Squared Error __(RMSE)__: 3789.30\n",
    "- R-Squared Score __(R2_Score)__: 0.83\n",
    "\n",
    "3. _`Linear Regression` with selected features and scaling_\n",
    "- Mean Absolute Error __(MAE)__: 2879.521\n",
    "- Root Mean Squared Error __(RMSE)__:3756.43\n",
    "- R-Squared Score __(R2_Score)__: 0.83\n",
    "\n",
    "4. _`Optimised Random Forest`_\n",
    "- Mean Absolute Error __(MAE)__: 3564.04\n",
    "- Root Mean Squared Error __(RMSE)__: 5915.82\n",
    "- R-Squared Score __(R2_Score)__: 0.73\n",
    "\n",
    "5. _`SVM` with feature scaling and selection_\n",
    "- Mean Absolute Error __(MAE)__: 7040.04\n",
    "- Root Mean Squared Error __(RMSE)__: 9794.59\n",
    "- R-Squared Score __(R2_Score)__: -0.16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
